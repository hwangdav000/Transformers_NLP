#Analysis of GPT and BERT based Transformer Models
For more detailed information you can find in my paper:
[Analysis on GPT and Bert Based Transformer Models](Transformer_Analysis.pdf)

Where I compared RoBERTa, BERT, GPT2, GPTNeo1.3B on NLP tasks sentiment analysis (SA), natural language inference (NLI), question answering (QA)

You can find the Datasets that I used for NLP tasks here:
</br>
SST (Sentiment Analysis):
</br>
https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews/data
</br>
</br>
SNLI (Natural Language Inference):
</br>
https://nlp.stanford.edu/projects/snli/
</br>
</br>
SQUAD2.0 (Question Answering):
</br>
https://rajpurkar.github.io/SQuAD-explorer/
</br>
</br>
Results 

![image](https://github.com/hwangdav000/Transformers_NLP/assets/29682356/0e80cfc6-ef42-4b2e-b6fd-ee9e842da45e)

I found that BERT performed best on SNLI (Natural Language Inference), RoBERTa performed best on SQUAD2.0 (Question Answer), and
GPT-NEO performed best on SST (Sentiment Analysis).
